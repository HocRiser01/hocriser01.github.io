<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="robots" content="noindex, nofollow">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.jpg">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"47.99.133.133","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="问题定义 输入两个 Tensor：\(input[Batch, Ci, Ih, Iw]\) （经典 NCHW 布局）、\(kernel[Co, Ci, Kh, Kw]\)（为适应 Coalesced Access 可重排为 \([Co, Kh, Kw, Ci]\)），其中前者为 FP32&#x2F;FP16 类型，后者为 INT4&#x2F;INT8 类型。输出卷积结果。 可以发现，对于 \(Iw &#x3D; Ih &#x3D; Ow">
<meta property="og:type" content="article">
<meta property="og:title" content="Per-block 量化混合精度卷积实现与 GEMM、GEMV 优化">
<meta property="og:url" content="http://47.99.133.133/2025/07/04/Per-block-%E9%87%8F%E5%8C%96%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E4%B8%8E-GEMM%E3%80%81GEMV-%E4%BC%98%E5%8C%96/index.html">
<meta property="og:site_name" content="朝汐">
<meta property="og:description" content="问题定义 输入两个 Tensor：\(input[Batch, Ci, Ih, Iw]\) （经典 NCHW 布局）、\(kernel[Co, Ci, Kh, Kw]\)（为适应 Coalesced Access 可重排为 \([Co, Kh, Kw, Ci]\)），其中前者为 FP32&#x2F;FP16 类型，后者为 INT4&#x2F;INT8 类型。输出卷积结果。 可以发现，对于 \(Iw &#x3D; Ih &#x3D; Ow">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-04T23:13:30.000Z">
<meta property="article:modified_time" content="2025-07-04T11:33:58.801Z">
<meta property="article:author" content="HocRiser">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://47.99.133.133/2025/07/04/Per-block-%E9%87%8F%E5%8C%96%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E4%B8%8E-GEMM%E3%80%81GEMV-%E4%BC%98%E5%8C%96/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Per-block 量化混合精度卷积实现与 GEMM、GEMV 优化 | 朝汐</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">朝汐</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">如星空般深蓝</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-gallery">

    <a href="/gallery/" rel="section"><i class="fa fa-image fa-fw"></i>Gallery</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/HocRiser01" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://47.99.133.133/2025/07/04/Per-block-%E9%87%8F%E5%8C%96%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%AE%9E%E7%8E%B0%E4%B8%8E-GEMM%E3%80%81GEMV-%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="HocRiser">
      <meta itemprop="description" content="平凡即是喜乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="朝汐">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Per-block 量化混合精度卷积实现与 GEMM、GEMV 优化
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-07-04 19:13:30 / Modified: 07:33:58" itemprop="dateCreated datePublished" datetime="2025-07-04T19:13:30-04:00">2025-07-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">笔记</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>31k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>57 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="问题定义">问题定义</h2>
<p>输入两个 Tensor：<span class="math inline">\(input[Batch, Ci, Ih,
Iw]\)</span> （经典 NCHW 布局）、<span class="math inline">\(kernel[Co,
Ci, Kh, Kw]\)</span>（为适应 Coalesced Access 可重排为 <span
class="math inline">\([Co, Kh, Kw, Ci]\)</span>），其中前者为 FP32/FP16
类型，后者为 INT4/INT8 类型。输出卷积结果。</p>
<p>可以发现，对于 <span class="math inline">\(Iw = Ih = Ow = Oh = Kw =
Kh = Dw = Dh = Sw = Sh = 1\)</span>（输入、输出、卷积核尺寸、扩张率
Dilute、步长 Stride 均为 1x1），<span class="math inline">\(Pw = Ph =
0\)</span>（不填充）的情况，卷积退化为标准卷积乘法 <span
class="math inline">\([Batch, Ci] \times [Ci, Co]\)</span>，以下简记为
<span class="math inline">\(A[N, K] \times B[K,
M]\)</span>。（当然，即使不是如此，也可以通过 Im2Col 或 Implicit Conv
等方式将其转换成可直接使用加速库的矩阵乘法）。</p>
<p>对于 <span class="math inline">\(B\)</span>（在 LLM
推理中通常为量化后权重矩阵），采用 Per-block 量化，即在 <span
class="math inline">\(K\)</span> 维度上，每 <span
class="math inline">\(block\_size\)</span> 个元素共享一套量化参数（<span
class="math inline">\(scale\)</span> 和 <span
class="math inline">\(zero\_point\)</span>），<span
class="math inline">\(M\)</span> 维度不共享。因此可以理解为，将 <span
class="math inline">\(B\)</span> 的每列按 <span
class="math inline">\(block\_size\)</span> 划分为多个竖条，总块数为
<span class="math inline">\(K / block\_size \times M\)</span>。</p>
<span id="more"></span>
<h2 id="初探">初探</h2>
<h3 id="conv_fpaint4b">CONV_FpAInt4B</h3>
<p>首先，卷积可以按照原始定义实现，我们对 <span
class="math inline">\(B\)</span>
在线反量化（离线反量化就失去量化本身的意义了），并要求一个量化块在同一个线程内处理从而共享量化参数。因此，我们让每个线程负责一个输出点，实现它的计算过程即可。</p>
<p>理论上有较大优化空间，但并非本文重点。INT4
需要加一个简单的解包，不再赘述。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">CONV_FpAInt4B</span><span class="params">(<span class="type">const</span> T* input,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">uint8_t</span>* kernel,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* scale, <span class="type">const</span> T* offset, <span class="type">const</span> T* bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    T *output,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> maxV, <span class="type">const</span> <span class="type">float</span> minV,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> ic, <span class="type">const</span> <span class="type">int</span> ic_p, <span class="type">const</span> <span class="type">int</span> iw, <span class="type">const</span> <span class="type">int</span> ih,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> c, <span class="type">const</span> <span class="type">int</span> c_p, <span class="type">const</span> <span class="type">int</span> ow, <span class="type">const</span> <span class="type">int</span> oh,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> kw, <span class="type">const</span> <span class="type">int</span> kh, <span class="type">const</span> <span class="type">int</span> dw, <span class="type">const</span> <span class="type">int</span> dh,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> sw, <span class="type">const</span> <span class="type">int</span> sh, <span class="type">const</span> <span class="type">int</span> pw, <span class="type">const</span> <span class="type">int</span> ph,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> total, <span class="type">const</span> <span class="type">int</span> quanC,</span></span></span><br><span class="line"><span class="params"><span class="function">    DivModFast d_oc, DivModFast d_ow, DivModFast d_oh</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; total; index += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        <span class="type">int</span> oz_2, tmp2, oy, ox, tmp1, ob;</span><br><span class="line">        d_oc.<span class="built_in">divmod</span>(index, tmp1, oz_2);</span><br><span class="line">        d_ow.<span class="built_in">divmod</span>(tmp1, tmp2, ox);</span><br><span class="line">        d_oh.<span class="built_in">divmod</span>(tmp2, ob, oy);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> oz = oz_2;</span><br><span class="line">        <span class="type">int</span> ix = ox * sw - pw;</span><br><span class="line">        <span class="type">int</span> iy = oy * sh - ph;</span><br><span class="line">        <span class="type">float</span> color0 = bias[oz];</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> num_quan_groups_per_channel = (c &gt; <span class="number">0</span> &amp;&amp; quanC &gt; <span class="number">0</span>) ? (quanC / c) : <span class="number">1</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> ic_per_group = (num_quan_groups_per_channel &gt; <span class="number">0</span>) ? (ic / num_quan_groups_per_channel) : ic;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> quan_param_index_base = oz * num_quan_groups_per_channel;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> fxSta = <span class="built_in">max</span>(<span class="number">0</span>, (<span class="built_in">UP_DIV</span>(-ix, dw)));</span><br><span class="line">        <span class="type">int</span> fySta = <span class="built_in">max</span>(<span class="number">0</span>, (<span class="built_in">UP_DIV</span>(-iy, dh)));</span><br><span class="line">        <span class="type">int</span> fxEnd = <span class="built_in">min</span>(kw, <span class="built_in">UP_DIV</span>(iw - ix, dw));</span><br><span class="line">        <span class="type">int</span> fyEnd = <span class="built_in">min</span>(kh, <span class="built_in">UP_DIV</span>(ih - iy, dh));</span><br><span class="line">        <span class="type">int</span> fx, fy, fz;</span><br><span class="line">        <span class="keyword">for</span> (fy=fySta; fy&lt;fyEnd; ++fy) &#123;</span><br><span class="line">            <span class="type">int</span> sy = fy*dh + iy;</span><br><span class="line">            <span class="keyword">for</span> (fx=fxSta; fx&lt;fxEnd; ++fx) &#123;</span><br><span class="line">                <span class="type">int</span> sx = fx*dw + ix;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> group_idx = <span class="number">0</span>; group_idx &lt; num_quan_groups_per_channel; ++group_idx) &#123;</span><br><span class="line">                    <span class="type">const</span> <span class="type">int</span> quan_param_index = quan_param_index_base + group_idx;</span><br><span class="line">                    <span class="type">const</span> <span class="type">float</span> x_scale = scale[quan_param_index];</span><br><span class="line">                    <span class="type">const</span> <span class="type">float</span> x_offset = offset[quan_param_index];</span><br><span class="line"></span><br><span class="line">                    <span class="type">const</span> <span class="type">int</span> sz_start = group_idx * ic_per_group / <span class="number">2</span>;</span><br><span class="line">                    <span class="type">const</span> <span class="type">int</span> sz_end = sz_start + ic_per_group / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> sz = sz_start; sz &lt; sz_end &amp;&amp; sz * <span class="number">2</span> &lt; ic_p; ++ sz) &#123;</span><br><span class="line">                        <span class="type">int</span> src_offset = ((ob * ih + sy) * iw + sx) * ic_p + <span class="number">2</span> * sz;</span><br><span class="line">                        <span class="type">float</span> inp0 = input[src_offset];</span><br><span class="line">                        <span class="type">float</span> inp1 = input[src_offset+<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">                        <span class="comment">//[Cop, KhKw, Cip]</span></span><br><span class="line">                        <span class="type">uint8_t</span> ker = kernel[((oz * kh + fy) * kw + fx) * ic_p / <span class="number">2</span> + sz];</span><br><span class="line">                        <span class="type">int8_t</span> ker0 = (ker &gt;&gt; <span class="number">4</span>) - <span class="number">8</span>;</span><br><span class="line">                        <span class="type">int8_t</span> ker1 = (ker &amp; <span class="number">15</span>) - <span class="number">8</span>;</span><br><span class="line">                        color0 = color0 + inp0 * ((<span class="type">float</span>)ker0 * x_scale + x_offset);</span><br><span class="line">                        color0 = color0 + inp1 * ((<span class="type">float</span>)ker1 * x_scale + x_offset);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        color0 = <span class="built_in">max</span>(color0, minV);</span><br><span class="line">        color0 = <span class="built_in">min</span>(color0, maxV);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> dst_offset = ((ob * oh + oy) * ow + ox) * c_p + oz;</span><br><span class="line"></span><br><span class="line">        output[dst_offset] = color0;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="convint8cutlassexecution">ConvInt8CutlassExecution</h3>
<p>接下来考察 GEMM 特殊情况，考虑先对 <span
class="math inline">\(A\)</span> 做量化，再对两个矩阵进行 INT*INT
的矩阵乘法，最后再将得到的 INT32 用一套新的量化参数量化。</p>
<p>先思考 <span class="math inline">\(A\)</span> 和 <span
class="math inline">\(B\)</span> 均为 Per-tensor
量化的特殊情况，即两个输入矩阵分别只有一套量化参数，输出有一套。此外还有一个
<span class="math inline">\(Bias\)</span> 参数需要加上。</p>
<p>将量化参数记为 <span class="math inline">\(s\)</span> 和 <span
class="math inline">\(z\)</span>，简单推导得：</p>
<p><span class="math inline">\(c_{fp, ij} = s_A s_B \left[
\sum_{k=1}^{K} a_{q, ik} b_{q, kj} - z_B \sum_{k=1}^{K} a_{q, ik} - z_A
\sum_{k=1}^{K} b_{q, kj} + K z_A z_B \right]\)</span></p>
<p>因此，以上算法是完全可行的，问题只有如何做好反量化和最终结果的量化。</p>
<p>考虑一个更简单的情况，假设 <span class="math inline">\(B\)</span>
为对称量化（即 <span class="math inline">\(Z_B =
0\)</span>）。下面将两个矩阵分别记为 <span
class="math inline">\(I\)</span> 和 <span
class="math inline">\(W\)</span>（Input 和
Weight）。本质上我们的操作流程如下：</p>
<ol type="1">
<li>首先反量化，将 INT8 的输入和权重变回浮点数：<span
class="math inline">\(I_{fp} = S_I \cdot (I_q - Z_I)\)</span>，<span
class="math inline">\(W_{fp} = S_W \cdot W_q\)</span>；</li>
<li>执行标准的浮点卷积和偏置加法：<span class="math inline">\(O_{fp} =
\text{GEMM}(I_{fp}, W_{fp}) + B_{fp}\)</span>；</li>
<li>重新量化，将浮点结果，变回 INT8 输出：<span
class="math inline">\(O_q = O_{fp} / S_O + Z_O\)</span>；</li>
</ol>
<p>我们的目标是跳过中间的浮点步骤。整合以上三个步骤得到：</p>
<p><span class="math inline">\(O_q = \frac{ \left( \sum S_I(I_q-Z_I)
\cdot S_W W_q \right) + B_{fp} }{S_O} + Z_O\)</span></p>
<p>分离出纯整数部分 <span class="math inline">\(\text{Accum}_{32} = \sum
I_q \cdot W_q\)</span>（32位整数累加器)： <span
class="math inline">\(O_q = \frac{S_I S_W \left( \sum I_q W_q - Z_I \sum
W_q \right) + B_{fp}}{S_O} + Z_O\)</span></p>
<p>提出总缩放因子 <span
class="math inline">\(M\)</span>，合并所有偏移项为 <span
class="math inline">\(\text{FusedBias}\)</span>：</p>
<p><span class="math inline">\(M = \frac{S_I \cdot S_W}{S_O}, \quad
\text{FusedBias} = -Z_I \cdot \sum W_q + \frac{B_{fp}}{S_I S_W} +
\frac{Z_O}{M}\)</span></p>
<p>得到最后结果：<span class="math inline">\(O_q = M \cdot
(\text{Accum}_{32} + \text{FusedBias})\)</span></p>
<p>代码实现：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> ConvInt8CutlassExecution::Resource::<span class="built_in">updateInputOutputScale</span>(std::vector&lt;<span class="type">float</span>&gt; inputQuantInfo, std::vector&lt;<span class="type">float</span>&gt; outputQuantInfo) &#123;</span><br><span class="line">    <span class="keyword">if</span>(mUseConvQuan) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// new scales and zero points</span></span><br><span class="line">    <span class="type">float</span> inputScale = inputQuantInfo[<span class="number">0</span>];</span><br><span class="line">    <span class="type">float</span> outputScale = outputQuantInfo[<span class="number">0</span>];</span><br><span class="line">    <span class="type">float</span> inputZeroPoint = inputQuantInfo[<span class="number">1</span>];</span><br><span class="line">    <span class="type">float</span> outputZeroPoint = outputQuantInfo[<span class="number">1</span>];</span><br><span class="line">    mClampMin = <span class="built_in">int8_t</span>(outputQuantInfo[<span class="number">2</span>]);</span><br><span class="line">    mClampMax = <span class="built_in">int8_t</span>(outputQuantInfo[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (inputScale == <span class="number">0.f</span> || outputScale == <span class="number">0.f</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    mInputScale = inputScale;</span><br><span class="line">    mOutputScale = outputScale;</span><br><span class="line">    mInputZeroPoint = <span class="built_in">int8_t</span>(inputZeroPoint);</span><br><span class="line">    mOutputZeroPoint = <span class="built_in">int8_t</span>(outputZeroPoint);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> kernelNum = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(mInt8WeightKernelSum.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> alphaScale  = inputScale / outputScale;</span><br><span class="line">    <span class="keyword">auto</span> alphaData = mScaleFloatVec;</span><br><span class="line">    <span class="keyword">auto</span> biasData = (<span class="type">float</span> *)mBiasInt32Vec;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; kernelNum; i++) &#123;</span><br><span class="line">        <span class="keyword">auto</span> alphaValue = alphaData[i];</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">fabs</span>(alphaValue) &lt; <span class="number">1e-6</span>) alphaValue = <span class="number">1e-6</span>;</span><br><span class="line">        mScaleFloatVec[i] = alphaValue * alphaScale;</span><br><span class="line">        <span class="comment">// compute outputZeroPointFused in asymmetric quant</span></span><br><span class="line">        <span class="type">int</span> outputZeroPointFused = <span class="built_in">static_cast</span>&lt;<span class="type">int32_t</span>&gt;(outputZeroPoint / mScaleFloatVec[i]);</span><br><span class="line">        mBiasInt32Vec[i] = <span class="built_in">static_cast</span>&lt;<span class="type">int32_t</span>&gt;(biasData[i] / (alphaScale * alphaValue)) - mInt8WeightKernelSum[i] * inputZeroPoint + outputZeroPointFused;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(M = \frac{S_I \cdot S_W}{S_O}\)</span>：
<code>mScaleFloatVec[i] = alphaValue * alphaScale</code>；</p>
<p><code>alphaValue</code> 对应权重尺度 <span
class="math inline">\(S_W\)</span>， <code>alphaScale</code> 对应<span
class="math inline">\(S_I / S_O\)</span>。</p>
<p><span class="math inline">\(\text{FusedBias} = -Z_I \sum W_q +
\frac{B_{fp}}{S_I S_W} +
\frac{Z_O}{M}\)</span>：<code>mBiasInt32Vec[i] = term1 + term2 + term3</code>;</p>
<p><code>term1 = biasData[i] / (alphaScale * alphaValue)</code> 对应
<span class="math inline">\(B_{fp} / M\)</span>。为了让它和公式中的
<span class="math inline">\(\frac{B_{fp}}{S_I S_W}\)</span>
匹配，biasData 必须是预先处理过的 <span
class="math inline">\(B_{fp}/S_O\)</span>。这是一个常见的实现技巧。</p>
<p><code>term2 = - mInt8WeightKernelSum[i] * inputZeroPoint</code> 对应
<span class="math inline">\(-Z_I \sum W_q\)</span>。</p>
<p><code>term3 = outputZeroPointFused</code> （即 outputZeroPoint /
mScaleFloatVec[i]） 对应公式中的 <span class="math inline">\(Z_O /
M\)</span>。</p>
<h3 id="llama.cpp-实现">llama.cpp 实现</h3>
<p>混合精度 GEMM 与 GEMV 代码重点在 ggml/src/ggml-cuda/ 下的
ggml-cuda.cu、mmq.cu、mmq.cuh、mmvq.cu。</p>
<p>量化相关代码重点在 quantize.cu 中；底层向量矩阵乘法实现在 vecdotq.cu
中。</p>
<p>Ilama.cpp 对于 FP16/FP32* INT 混合精度矩阵乘法：</p>
<ol type="1">
<li>cuBLAS 路径：全反量化成 FP16/FP32（代码在 ggml-cuda.cu 中的
<code>ggml_cuda_mul_mat_batched_cublas</code>、
<code>ggml_cuda_op_mul_mat_cublas</code>）；</li>
<li>手写内核（<a
target="_blank" rel="noopener" href="https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#cuda">MMQ</a>）路径：
<ol type="1">
<li>对 FP16<em>INT，使用cuBLAS 路径，执行 FP16</em> FP16，再将 FP16
结果反量化成 FP32；</li>
<li>对 FP32 * INT，全矩阵量化成 INT，执行 INT *INT，再将 INT32
结果反量化为 FP32；
<ol type="1">
<li>分muL_mat_q 和 mul_mat_vec_q 两个版本；还有对 MoE 特化版本；</li>
<li>没有能够直接调用的在线反量化并做矩阵乘的 cuBLAS 接口；没用
Cutlass；</li>
<li>大量模板元实现的编译期分支，用于确定核函数常数和调用的函数指针；</li>
</ol></li>
</ol></li>
</ol>
<p>以下是几个与 MMQ 路径相关的关键函数：</p>
<ul>
<li><strong><code>ggml_cuda_op_mul_mat</code></strong>:
一个通用的矩阵乘法执行引擎，它负责处理多GPU数据切分与同步，并能通过函数指针调用任何具体计算实现（cuBLAS或自定义量化核函数）。</li>
<li><strong><code>ggml_cuda_mul_mat</code></strong>:
矩阵乘法操作的顶层“决策者”，它通过分析输入张量的类型、形状和硬件特性，智能地分发任务给最优的后端实现（如自定义量化内核或多种cuBLAS路径）。
在需要 GPU 切分等时调用 ggml_cuda_op_mul_mat，否则直接调用
ggml_cuda_mul_mat_q 等；</li>
<li><strong><code>ggml_cuda_mul_mat_q</code></strong>:
通用“矩阵-矩阵”量化乘法（MMQ）的逻辑主入口，负责动态量化FP32输入并处理标准及混合专家（MoE）两种计算模式。
针对 MoE 的整体解决方案。</li>
<li><strong><code>ggml_cuda_op_mul_mat_q</code></strong>:
作为通用执行引擎调用的底层计算接口，它接收已准备好的量化输入，将其打包为内核参数并启动实际的“矩阵-矩阵”量化计算核函数。
直接得到上层 ggml_cuda_op_mul_mat 量化处理之后的参数。</li>
</ul>
<p><code>mul_mat_q_case</code> 是真正的执行函数。</p>
<h2 id="方案一在线反量化">方案一：在线反量化</h2>
<p>现在，我们考虑上一节的在线反量化卷积的矩阵乘法与向量-矩阵乘法特化版本。</p>
<p>根据 CUDA 编程加速技巧，我们对 GEMM 做如下优化：</p>
<ol type="1">
<li>将矩阵分割为 16x16 的 Tile，每个 thread-block 负责结果的一块（涉及 A
的一”块行“ 与 B 的一”块列“）；每个线程负责其中 A 的一行与 B 的一列，
<code>k_tile</code> 负责枚举 K 维度上的块；</li>
<li>代码中矩阵乘法的过程访问的是 <code>B_tile_fp[k][tx]</code> 和
<code>A_tile[ty][k]</code> 。相邻线程 ty 相等， tx 相邻。因此同一个 warp
内，B 访问的是同一行数据（合并访问），而 A
访问的是同一个数（直接触发广播）。均不会发生 32-way bank
conflict，因此对 B_tile 列维度 +1 的 Padding 是没有必要的；</li>
<li>GEMM 循环内第一阶段，通过合并访存将 A 和 B 载入到 Shared Memory
中；第二阶段并行进行反量化与转置；第三阶段做矩阵乘法；</li>
</ol>
<p>代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> TILE_DIM = <span class="number">16</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">GEMM_FpAInt8B</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* input,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int8_t</span>* kernel,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* scale, <span class="type">const</span> T* offset, <span class="type">const</span> T* bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    T* output,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> maxV, <span class="type">const</span> <span class="type">float</span> minV,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> ic, <span class="type">const</span> <span class="type">int</span> ic_p, <span class="type">const</span> <span class="type">int</span> oc, <span class="type">const</span> <span class="type">int</span> oc_p,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> batch, <span class="type">const</span> <span class="type">int</span> quanC</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    __shared__ T A_tile[TILE_DIM][TILE_DIM]; <span class="comment">// [batch, ic]</span></span><br><span class="line">    __shared__ <span class="type">int8_t</span> B_tile_s8[TILE_DIM][TILE_DIM]; <span class="comment">// [ic, oc] kernel 本身是 B^T [oc, ic]</span></span><br><span class="line">    __shared__ T B_tile_fp[TILE_DIM][TILE_DIM];</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_row = blockIdx.y; <span class="comment">// batch</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_col = blockIdx.x; <span class="comment">// output channel</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个线程负责计算输出Tile中的一个元素</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> out_row = block_row * TILE_DIM + ty; <span class="comment">// M</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> out_col = block_col * TILE_DIM + tx; <span class="comment">// N</span></span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> acc = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="comment">// 沿K维度（输入通道ic）分块循环</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_k_tiles = <span class="built_in">UP_DIV</span>(ic, TILE_DIM);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k_tile = <span class="number">0</span>; k_tile &lt; num_k_tiles; ++k_tile) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> k_tile_base = k_tile * TILE_DIM;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 合并访问加载 A_tile (input)，线程 (ty, tx) 加载 A_tile[ty][tx]</span></span><br><span class="line">        <span class="type">int</span> a_col_idx = k_tile_base + tx;</span><br><span class="line">        A_tile[ty][tx] = (out_row &lt; batch &amp;&amp; a_col_idx &lt; ic) ? input[out_row * ic_p + a_col_idx] : (T)<span class="number">0.0f</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 合并访问加载 B_tile (kernel)，线程 (ty, tx) 加载 B_tile[ty][tx]</span></span><br><span class="line">        <span class="comment">// kernel 布局为 [oc, ic]，需要 B(k,n)，即 kernel(n,k)</span></span><br><span class="line">        <span class="type">int</span> b_load_row = block_col * TILE_DIM + ty;</span><br><span class="line">        <span class="type">int</span> b_col_idx = k_tile_base + tx;</span><br><span class="line">        B_tile_s8[ty][tx] = (b_load_row &lt; oc &amp;&amp; b_col_idx &lt; ic) ? kernel[b_load_row * ic_p + b_col_idx] : <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 反量化 + 转置</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> K = ty;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> N = tx;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> global_k = k_tile_base + K;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> global_n = block_col * TILE_DIM + N;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (global_n &lt; oc &amp;&amp; global_k &lt; ic) &#123;</span><br><span class="line">            <span class="type">const</span> <span class="type">int</span> num_quan_groups_per_channel = (quanC &gt; <span class="number">0</span>) ? (quanC / oc) : <span class="number">1</span>;</span><br><span class="line">            <span class="type">const</span> <span class="type">int</span> ic_per_group = (num_quan_groups_per_channel &gt; <span class="number">0</span>) ? (ic / num_quan_groups_per_channel) : ic;</span><br><span class="line">            <span class="type">const</span> <span class="type">int</span> group_idx = global_k / ic_per_group;</span><br><span class="line">            <span class="type">const</span> <span class="type">int</span> quan_param_index = global_n * num_quan_groups_per_channel + group_idx;</span><br><span class="line"></span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x_scale  = (<span class="type">float</span>)scale[quan_param_index];</span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> x_offset = (<span class="type">float</span>)offset[quan_param_index];</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// B_tile_s8(n,k), thread(n,k) -&gt; (tx, ty)</span></span><br><span class="line">            <span class="comment">// So we need to read from B_tile_s8[n_dim_in_tile][k_dim_in_tile] -&gt; B_tile_s8[tx][ty]</span></span><br><span class="line">            <span class="type">const</span> <span class="type">float</span> b_quant = (<span class="type">float</span>)B_tile_s8[N][K];</span><br><span class="line"></span><br><span class="line">            <span class="comment">// B_tile_fp[k][n] -&gt; B_tile_fp[ty][tx]</span></span><br><span class="line">            B_tile_fp[K][N] = (T)(b_quant * x_scale + x_offset);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在 SMEM 中进行子矩阵乘法</span></span><br><span class="line">        <span class="keyword">if</span> (out_col &lt; oc) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; TILE_DIM; ++k) &#123;</span><br><span class="line">                acc += (<span class="type">float</span>)A_tile[ty][k] * (<span class="type">float</span>)B_tile_fp[k][tx];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写回全局内存</span></span><br><span class="line">    <span class="keyword">if</span> (out_row &lt; batch &amp;&amp; out_col &lt; oc) &#123;</span><br><span class="line">        acc += (<span class="type">float</span>)bias[out_col];</span><br><span class="line">        acc = <span class="built_in">max</span>(acc, minV);</span><br><span class="line">        acc = <span class="built_in">min</span>(acc, maxV);</span><br><span class="line">        output[out_row * oc_p + out_col] = (T)acc;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用</span></span><br><span class="line"><span class="function">dim3 <span class="title">threads</span><span class="params">(TILE_DIM, TILE_DIM)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">blocks</span><span class="params">(UP_DIV(ocp, TILE_DIM), UP_DIV(batch, TILE_DIM))</span></span>;</span><br></pre></td></tr></table></figure>
<p>对 GEMV 做如下优化：</p>
<ol type="1">
<li>每个线程块负责计算一个输出位置，每个线程负责一个 %64
剩余系的位置（合并访存+计算）；</li>
<li>由于向量-矩阵乘的结果是一个向量，因此直接做并行规约（蝶式交换）即可，最终只需要
thread0 写回 Global Memory；</li>
<li>使用动态 Shared Memory，直接在第一阶段通过合并访存从 Global Memory
中加载；</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">GEMV_FpAInt4B</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* input,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">uint8_t</span>* kernel, <span class="comment">// kernel 是打包的 int4</span></span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* scale, <span class="type">const</span> T* offset, <span class="type">const</span> T* bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    T* output,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> maxV, <span class="type">const</span> <span class="type">float</span> minV,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> ic, <span class="type">const</span> <span class="type">int</span> ic_p,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> oc, <span class="type">const</span> <span class="type">int</span> quanC</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">uint8_t</span> smem_buffer[];</span><br><span class="line">    T* smem_input = <span class="built_in">reinterpret_cast</span>&lt;T*&gt;(smem_buffer);</span><br><span class="line">    <span class="type">float</span>* partial_sums = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span>*&gt;(smem_buffer + ic_p * <span class="built_in">sizeof</span>(T));</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> oz = blockIdx.x; <span class="comment">// 当前线程块负责计算的输出通道索引</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 加载 input 到共享内存</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = tid; i &lt; ic; i += blockDim.x) smem_input[i] = input[i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = ic + tid; i &lt; ic_p; i += blockDim.x) smem_input[i] = (T)<span class="number">0.0f</span>;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_quan_groups_per_channel = (quanC &gt; <span class="number">0</span>) ? (quanC / oc) : <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ic_per_group = (num_quan_groups_per_channel &gt; <span class="number">0</span>) ? (ic / num_quan_groups_per_channel) : ic;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> quan_param_index_base = oz * num_quan_groups_per_channel;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> my_sum = <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = tid * <span class="number">2</span>; k &lt; ic; k += blockDim.x * <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">// 加载打包的权重并解包</span></span><br><span class="line">        <span class="type">const</span> <span class="type">uint8_t</span> ker_packed = kernel[oz * (ic_p / <span class="number">2</span>) + k / <span class="number">2</span>];</span><br><span class="line">        <span class="type">const</span> <span class="type">int8_t</span> ker0_s8 = (ker_packed &gt;&gt; <span class="number">4</span>) - <span class="number">8</span>;</span><br><span class="line">        <span class="type">const</span> <span class="type">int8_t</span> ker1_s8 = (ker_packed &amp; <span class="number">0x0F</span>) - <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> group_idx0 = k / ic_per_group;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> quan_param_index0 = quan_param_index_base + group_idx0;</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> x_scale0  = scale[quan_param_index0];</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> x_offset0 = offset[quan_param_index0];</span><br><span class="line">        my_sum += (<span class="type">float</span>)smem_input[k] * ((<span class="type">float</span>)ker0_s8 * x_scale0 + x_offset0);</span><br><span class="line">        my_sum += (<span class="type">float</span>)smem_input[k + <span class="number">1</span>] * ((<span class="type">float</span>)ker1_s8 * x_scale0 + x_offset0);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    partial_sums[tid] = my_sum;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 并行规约</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> s = blockDim.x / <span class="number">2</span>; s &gt; <span class="number">0</span>; s &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (tid &lt; s) partial_sums[tid] += partial_sums[tid + s];</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="type">float</span> final_val = partial_sums[<span class="number">0</span>] + (<span class="type">float</span>)bias[oz];</span><br><span class="line">        final_val = <span class="built_in">max</span>(final_val, minV);</span><br><span class="line">        final_val = <span class="built_in">min</span>(final_val, maxV);</span><br><span class="line">        output[oz] = (T)final_val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用</span></span><br><span class="line"><span class="function">dim3 <span class="title">threads</span><span class="params">(GEMV_TILE)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">blocks</span><span class="params">(oc)</span></span>;</span><br><span class="line"><span class="type">size_t</span> input_smem_size = icp * (mFp16Infer ? <span class="built_in">sizeof</span>(half) : <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"><span class="type">size_t</span> reduction_smem_size = GEMV_TILE * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="type">size_t</span> smem_size = input_smem_size + reduction_smem_size;</span><br><span class="line">GEMV_FpAInt4B&lt;&lt;&lt;blocks, threads, smem_size&gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<h2 id="方案二在线量化">方案二：在线量化</h2>
<h3 id="方案推导">方案推导</h3>
<p>最后，我们考虑对 <span class="math inline">\(A\)</span> 做与 <span
class="math inline">\(B\)</span> 相同 <span
class="math inline">\(block\_size\)</span> 的 Per-block 量化（均在 <span
class="math inline">\(K\)</span> 维度切割），再调用 INT*INT
矩阵乘法，最后反量化得到浮点结果。现在，<span
class="math inline">\(A\)</span> 的每行被分割，<span
class="math inline">\(B\)</span> 的每列被分割，其余维度不共享参数。</p>
<p>问题在于，不同 block 中的元素使用的是不同的
量化参数，因此显然无法只通过一次完整的整数矩阵乘法就得到答案。另一方面，逐个
block 地相乘效率又太低，无法享受到 Cutlass 的加速（一个 block 往往只有
64 个元素）。</p>
<p>因此，我们考虑每次拿出 <span class="math inline">\(A\)</span>
的一”块列“，与 <span class="math inline">\(B\)</span>
的一”块行“做矩阵乘法，得到一个 <span class="math inline">\(N \times
M\)</span> 的矩阵，然后暴力将矩阵中的每个数（对应一对 block
的乘积）用对应那对 block 的量化参数做反量化，加到答案矩阵上。重复 <span
class="math inline">\(\frac{K}{block\_size}\)</span> 次即可。</p>
<p>更形式化地：</p>
<p>我们将求和维度 <span class="math inline">\(K\)</span> 分为 <span
class="math inline">\(P\)</span> 个连续的块（block）：<span
class="math inline">\(K = G_1 \cup G_2 \cup \dots \cup
G_P\)</span>。</p>
<p>量化参数 <span class="math inline">\(s\)</span> 和 <span
class="math inline">\(z\)</span> 不仅依赖于矩阵和行列，更依赖于 <span
class="math inline">\(k\)</span> 所在的块 <span
class="math inline">\(G_i\)</span>。</p>
<p>因此，反量化公式应该写成（下标 <span class="math inline">\(i\)</span>
代表第 <span class="math inline">\(i\)</span> 个 <span
class="math inline">\(K\)</span> 维度块）：</p>
<p><span class="math inline">\(A_{fp, mk} = s_{A,m,i} \cdot (A_{q, mk} -
z_{A,m,i}) \quad k \in G_i\)</span></p>
<p><span class="math inline">\(B_{fp, kn} = s_{B,n,i} \cdot (B_{q, kn} -
z_{B,n,i}) \quad k \in G_i\)</span></p>
<p>代入矩阵乘法公式（ <span class="math inline">\(g(k)\)</span> 为索引
<span class="math inline">\(k\)</span> 所属的块的编号）：</p>
<p><span class="math inline">\(C_{fp, mn} = \sum_{k=1}^{K} \left[
s_{A,m,g(k)} \cdot (A_{q, mk} - z_{A,m,g(k)}) \right] \cdot \left[
s_{B,n,g(k)} \cdot (B_{q, kn} - z_{B,n,g(k)}) \right]\)</span></p>
<p>由于缩放因子 <span class="math inline">\(s_A, s_B\)</span> 和零点
<span class="math inline">\(z_A, z_B\)</span> 的值会随着 <span
class="math inline">\(k\)</span>
的变化而改变。因此不能将它们作为常数从整个求和 <span
class="math inline">\(\sum_{k=1}^{K}\)</span> 中提出来。
改变求和结构，将对 <span class="math inline">\(k\)</span>
的总求和分解为“对所有块的求和”，内部嵌套“对块内元素的求和”： <span
class="math inline">\(C_{fp, mn} = \sum_{i=1}^{P} \left( \sum_{k \in
G_i} \left[ s_{A,m,i} \cdot (A_{q, mk} - z_{A,m,i}) \right] \cdot \left[
s_{B,n,i} \cdot (B_{q, kn} - z_{B,n,i}) \right] \right)\)</span></p>
<p>对于求和范围 <span class="math inline">\(\sum_{k \in G_i}\)</span>
内的所有 <span class="math inline">\(k\)</span>，它们都属于同一个块
<span
class="math inline">\(G_i\)</span>，因此它们的量化参数不变，可以将这些参数提到内部求和的外面：
<span class="math inline">\(C_{fp, mn} = \sum_{i=1}^{P} s_{A,m,i} \cdot
s_{B,n,i} \left( \sum_{k \in G_i} (A_{q, mk} - z_{A,m,i}) \cdot (B_{q,
kn} - z_{B,n,i}) \right)\)</span></p>
<p>以上即逐 block 乘法的数学基础，接下来推导”块行块列“乘法：
我们定义一个<strong>第 <span class="math inline">\(i\)</span>
块的浮点结果矩阵</strong>：</p>
<p><span class="math inline">\(C_{fp, mn}^{(i)} := s_{A,m,i} \cdot
s_{B,n,i} \left( \sum_{k \in G_i} (A_{q, mk} - z_{A,m,i}) \cdot (B_{q,
kn} - z_{B,n,i}) \right)\)</span></p>
<p>那么，最终的输出矩阵 <span class="math inline">\(C_{fp}\)</span>
就是所有这些局部结果矩阵的简单叠加： <span class="math inline">\(C_{fp,
mn} = \sum_{i=1}^{P} C_{fp, mn}^{(i)}\)</span></p>
<p>定义与块 <span class="math inline">\(i\)</span> 相关的子矩阵：</p>
<p><span class="math inline">\(A_q^{(i)}\)</span>：由矩阵 <span
class="math inline">\(A_q\)</span> 的所有行，以及 <strong>只属于块 <span
class="math inline">\(G_i\)</span> 的列</strong> 构成的子矩阵。维度为
<span class="math inline">\(M \times block\_size\)</span>；</p>
<p><span class="math inline">\(B_q^{(i)}\)</span>：由矩阵 <span
class="math inline">\(B_q\)</span> 的所有列，以及 <strong>只属于块 <span
class="math inline">\(G_i\)</span> 的行</strong> 构成的子矩阵。其维度为
<span class="math inline">\(block\_size \times N\)</span>；</p>
<p>这两个矩阵的乘积结果定义为 <strong>第 <span
class="math inline">\(i\)</span> 块的整数结果矩阵</strong> <span
class="math inline">\(C_{q}^{(i)}\)</span>： <span
class="math inline">\(C_{q, mn}^{(i)} := \sum_{k \in G_i} A_{q, mk}
B_{q, kn}\)</span></p>
<p>这个运算是一个维度为 <span class="math inline">\((M \times
block\_size) \times (block\_size \times N) \rightarrow (M \times
N)\)</span> 的通用矩阵乘法 (GEMM)。其结果 <span
class="math inline">\(C_{q}^{(i)}\)</span> 是一个完整的 <span
class="math inline">\(M \times N\)</span> 的 INT32 矩阵。</p>
<p>再次展开 <span class="math inline">\(C_{fp, mn}^{(i)}\)</span>
的定义并将 <span class="math inline">\(C_{q, mn}^{(i)}\)</span>
代入，我们得到了最终的反量化公式，它描述了如何将一个完整的中间整数矩阵逐元素地转换为浮点矩阵：</p>
<p><span class="math inline">\(C_{fp, mn}^{(i)} = s_{A,m,i} s_{B,n,i}
\cdot \\ \left( \underbrace{\sum_{k \in G_i} A_{q, mk}B_{q, kn}}_{C_{q,
mn}^{(i)}} - z_{B,n,i}\sum_{k \in G_i} A_{q, mk} - z_{A,m,i}\sum_{k \in
G_i} B_{q, kn} + block\_size \cdot z_{A,m,i}z_{B,n,i}
\right)\)</span></p>
<p>算法流程如下：</p>
<p>for i = 1 to P（遍历所有 <span class="math inline">\(K\)</span>
维度的块）：</p>
<ol type="1">
<li>整数子矩阵乘法
<ol type="1">
<li>提取子矩阵 <span class="math inline">\(A_q^{(i)}\)</span> 和 <span
class="math inline">\(B_q^{(i)}\)</span>。</li>
<li>调用高效 int8*int8 -&gt; int32 GEMM 库函数，计算出中间结果矩阵 <span
class="math inline">\(C_{q_i} = \text{matmul}(A_{q_i},
B_{q_i})\)</span>。</li>
</ol></li>
<li>逐元素反量化与累加
<ol type="1">
<li>启动一个 CUDA Kernel，每个线程处理 <span
class="math inline">\(C_{q_i}\)</span> 的一个或多个元素。</li>
<li>在 Kernel 内部，对于每个元素 <span class="math inline">\((m,
n)\)</span>：
<ol type="1">
<li>读取 <span class="math inline">\(C_{q_i,mn}\)</span>，根据块 <span
class="math inline">\(i\)</span> 的量化参数 <span
class="math inline">\(s_{A,m,i}, z_{A,m,i}, s_{B,n,i},
z_{B,n,i}\)</span>，计算出局部浮点值 <span
class="math inline">\(c_{fp_i}\)</span>。</li>
<li>以原子方式或直接（如果输出空间不冲突）将 <span
class="math inline">\(c_{fp_i}\)</span>
加到最终结果矩阵的对应位置。</li>
</ol></li>
</ol></li>
</ol>
<p>以下代码采用 offset 而非 zero-point 形式：</p>
<p><span class="math inline">\(C_{fp, mn}^{(i)} = \sum_{k \in G_i}
\left[ \text{scale}_{A,m,i} A_{q,mk} + \text{offset}_{A,m,i} \right]
\cdot \left[ \text{scale}_{B,n,i} B_{q,kn} + \text{offset}_{B,n,i}
\right] \\ = \left(\text{scale}_{A,m,i}\text{scale}_{B,n,i}\right)
C_{q,mn}^{(i)} + \left(\text{scale}_{A,m,i}\text{offset}_{B,n,i}\right)
A_{q,m,i} \\ + \left(\text{offset}_{A,m,i}\text{scale}_{B,n,i}\right)
B_{q,n,i} + K_i \cdot
\text{offset}_{A,m,i}\text{offset}_{B,n,i}\)</span></p>
<h3 id="代码">代码</h3>
<p>对于核函数部分，做了以下优化：</p>
<ol type="1">
<li>使用 union 复用共享内存，减少总占用量；</li>
<li>对于并行规约，先试用 <code>__shfl_down_sync</code> 做 Warp
内规约（原子操作），再在 Warp 0 做一次 Warp 间规约（实际上仍是
<code>__shfl_down_sync</code> 做 Warp 内规约）；</li>
<li>查看 cuda_fp16.hpp（CUDA Toolkit
12.4）可知，<code>__half atomicAdd(__half *const address, const __half val)</code>
需要在 SM_70
及以后才支持，因此使用预编译期条件（这种方式同样可以应对胖二进制（Fat
Binary）编译方式的情况）
<code>#if (defined(__CUDACC__) &amp;&amp; (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ &gt;= 700))) || defined(_NVHPC_CUDA)</code>
和编译期条件 <code>if constexpr (std::is_same_v&lt;T, half&gt;)</code>
进行分支判断；</li>
<li>Cutlass 可以直接调用，也可以分成准备+执行两阶段（在下文 Cutlass
相关代码中）；</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 动态量化 A、计算 sum_A</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">QuantA</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* A_sub_fp, <span class="type">int8_t</span>* A_sub_q,</span></span></span><br><span class="line"><span class="params"><span class="function">    T* scale_A_out, T* offset_A_out, <span class="type">int32_t</span>* sum_A_q_out,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> K_i, <span class="type">const</span> <span class="type">int</span> lda</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用 union 复用共享内存，减少总占用量</span></span><br><span class="line">    __shared__ <span class="keyword">union</span> &#123;</span><br><span class="line">        <span class="type">float</span> min_max_vals[<span class="number">2</span>][BLOCK_SIZE / WARP_SIZE]; <span class="comment">// 用于 min/max 的 warp 间规约</span></span><br><span class="line">        <span class="type">int32_t</span> sum_vals[BLOCK_SIZE / WARP_SIZE];      <span class="comment">// 用于 sum 的 warp 间规约</span></span><br><span class="line">        <span class="type">float</span> scale_offset[<span class="number">2</span>];                        <span class="comment">// 用于向块内所有线程广播 scale 和 offset</span></span><br><span class="line">    &#125; smem;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> m = blockIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (m &gt;= M) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warp_id = tid / WARP_SIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> lane_id = tid % WARP_SIZE;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> my_min = FLT_MAX;</span><br><span class="line">    <span class="type">float</span> my_max = -FLT_MAX;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = tid; k &lt; K_i; k += BLOCK_SIZE) &#123;</span><br><span class="line">        <span class="type">float</span> val = (<span class="type">float</span>)A_sub_fp[m * lda + k];</span><br><span class="line">        my_min = <span class="built_in">min</span>(my_min, val);</span><br><span class="line">        my_max = <span class="built_in">max</span>(my_max, val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Warp 内规约：使用 __shfl_down_sync 在 warp 内无锁计算 min/max</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = WARP_SIZE / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        my_min = <span class="built_in">min</span>(my_min, __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_min, offset));</span><br><span class="line">        my_max = <span class="built_in">max</span>(my_max, __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_max, offset));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Warp 间规约：每个 warp 的 0 号线程将 warp 结果写入共享内存</span></span><br><span class="line">    <span class="keyword">if</span> (lane_id == <span class="number">0</span>) &#123;</span><br><span class="line">        smem.min_max_vals[<span class="number">0</span>][warp_id] = my_min;</span><br><span class="line">        smem.min_max_vals[<span class="number">1</span>][warp_id] = my_max;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (warp_id == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// 从共享内存加载各 warp 的结果</span></span><br><span class="line">        <span class="keyword">if</span> (lane_id &lt; (BLOCK_SIZE / WARP_SIZE)) &#123;</span><br><span class="line">            my_min = smem.min_max_vals[<span class="number">0</span>][lane_id];</span><br><span class="line">            my_max = smem.min_max_vals[<span class="number">1</span>][lane_id];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            my_min = FLT_MAX;</span><br><span class="line">            my_max = -FLT_MAX;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 在第一个 warp 内部完成最终规约</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> offset = (BLOCK_SIZE / WARP_SIZE) / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">            my_min = <span class="built_in">min</span>(my_min, __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_min, offset));</span><br><span class="line">            my_max = <span class="built_in">max</span>(my_max, __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_max, offset));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 线程 0 计算 scale/offset 并写入共享内存进行广播</span></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="type">float</span> scale = (my_max - my_min) / <span class="number">255.0f</span>;</span><br><span class="line">        <span class="type">float</span> offset;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(scale) &gt; <span class="number">1e-5</span>) &#123;</span><br><span class="line">            offset = my_max - scale * <span class="number">127.0f</span>; <span class="comment">// my_max 映射到 127, my_min 映射到 -128</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            scale = <span class="number">1.0f</span>;</span><br><span class="line">            offset = my_max;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        scale_A_out[m] = (T)scale;</span><br><span class="line">        offset_A_out[m] = (T)offset;</span><br><span class="line">        </span><br><span class="line">        smem.scale_offset[<span class="number">0</span>] = scale;</span><br><span class="line">        smem.scale_offset[<span class="number">1</span>] = offset;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 所有线程从共享内存读取 scale/offset, 并进行量化和求和</span></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> s_scale_A = smem.scale_offset[<span class="number">0</span>];</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> s_offset_A = smem.scale_offset[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="type">int32_t</span> my_sum_q = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = tid; k &lt; K_i; k += BLOCK_SIZE) &#123;</span><br><span class="line">        <span class="type">float</span> val_fp = (<span class="type">float</span>)A_sub_fp[m * lda + k];</span><br><span class="line">        <span class="type">int32_t</span> val_q = <span class="built_in">roundf</span>((val_fp - s_offset_A) / s_scale_A);</span><br><span class="line">        <span class="type">int8_t</span> a_q = (<span class="type">int8_t</span>)<span class="built_in">max</span>(<span class="number">-128</span>, <span class="built_in">min</span>(<span class="number">127</span>, val_q));</span><br><span class="line">        A_sub_q[m * K_i + k] = a_q;</span><br><span class="line">        my_sum_q += a_q;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = WARP_SIZE / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">        my_sum_q += __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_sum_q, offset);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (lane_id == <span class="number">0</span>) smem.sum_vals[warp_id] = my_sum_q;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (warp_id == <span class="number">0</span>) &#123;</span><br><span class="line">        my_sum_q = (lane_id &lt; (BLOCK_SIZE / WARP_SIZE)) ? smem.sum_vals[lane_id] : <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> offset = (BLOCK_SIZE / WARP_SIZE) / <span class="number">2</span>; offset &gt; <span class="number">0</span>; offset &gt;&gt;= <span class="number">1</span>) &#123;</span><br><span class="line">             my_sum_q += __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, my_sum_q, offset);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 线程 0 将最终的和写入全局内存</span></span><br><span class="line">    <span class="keyword">if</span> (tid == <span class="number">0</span>) sum_A_q_out[m] = my_sum_q;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">GEMM_Int8</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int8_t</span>* A_q, <span class="type">const</span> <span class="type">int8_t</span>* B_q, <span class="type">int32_t</span>* C_q,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K_i,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> lda_q, <span class="type">const</span> <span class="type">int</span> ldb, <span class="type">const</span> <span class="type">int</span> ldc</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int8_t</span> A_tile_s8[SUB_GEMM_TILE_DIM][SUB_GEMM_TILE_DIM];</span><br><span class="line">    __shared__ <span class="type">int8_t</span> B_tile_s8[SUB_GEMM_TILE_DIM][SUB_GEMM_TILE_DIM];</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_row = blockIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_col = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> row = block_row * SUB_GEMM_TILE_DIM + ty;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> col = block_col * SUB_GEMM_TILE_DIM + tx;</span><br><span class="line"></span><br><span class="line">    <span class="type">int32_t</span> acc = <span class="number">0</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_k_tiles = <span class="built_in">UP_DIV</span>(K_i, SUB_GEMM_TILE_DIM);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k_tile = <span class="number">0</span>; k_tile &lt; num_k_tiles; ++k_tile) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> k_tile_base = k_tile * SUB_GEMM_TILE_DIM;</span><br><span class="line"></span><br><span class="line">        A_tile_s8[ty][tx] = (row &lt; M &amp;&amp; (k_tile_base + tx) &lt; K_i) ? A_q[row * lda_q + k_tile_base + tx] : <span class="number">0</span>;</span><br><span class="line">        B_tile_s8[ty][tx] = (col &lt; N &amp;&amp; (k_tile_base + ty) &lt; K_i) ? B_q[col * ldb + k_tile_base + ty] : <span class="number">0</span>;</span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; SUB_GEMM_TILE_DIM; ++k) &#123;</span><br><span class="line">            acc += (<span class="type">int32_t</span>)A_tile_s8[ty][k] * (<span class="type">int32_t</span>)B_tile_s8[k][tx];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (row &lt; M &amp;&amp; col &lt; N) C_q[row * ldc + col] = acc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">DequantAndAcc</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int32_t</span>* C_q, T* C_fp_final,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* scale_A_in, <span class="type">const</span> T* offset_A_in,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> T* base_scale_B, <span class="type">const</span> T* base_offset_B,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int32_t</span>* base_sum_B_q,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> group_idx, <span class="type">const</span> <span class="type">int</span> num_oc_groups,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int32_t</span>* sum_A_q_in,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> K_i, <span class="type">const</span> <span class="type">int</span> ldc</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 使用共享内存缓存行和列的量化参数</span></span><br><span class="line">    __shared__ <span class="type">float</span> smem_scale_A[DEQUANT_TILE_DIM];</span><br><span class="line">    __shared__ <span class="type">float</span> smem_offset_A[DEQUANT_TILE_DIM];</span><br><span class="line">    __shared__ <span class="type">int32_t</span> smem_sum_A_q[DEQUANT_TILE_DIM];</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> smem_scale_B[DEQUANT_TILE_DIM];</span><br><span class="line">    __shared__ <span class="type">float</span> smem_offset_B[DEQUANT_TILE_DIM];</span><br><span class="line">    __shared__ <span class="type">int32_t</span> smem_sum_B_q[DEQUANT_TILE_DIM];</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_row_start = blockIdx.y * DEQUANT_TILE_DIM;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> block_col_start = blockIdx.x * DEQUANT_TILE_DIM;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个线程加载一个 A 的参数</span></span><br><span class="line">    <span class="type">int</span> m_load_idx = block_row_start + ty;</span><br><span class="line">    <span class="keyword">if</span> (tx == <span class="number">0</span> &amp;&amp; m_load_idx &lt; M) &#123;</span><br><span class="line">        smem_scale_A[ty]   = (<span class="type">float</span>)scale_A_in[m_load_idx];</span><br><span class="line">        smem_offset_A[ty]  = (<span class="type">float</span>)offset_A_in[m_load_idx];</span><br><span class="line">        smem_sum_A_q[ty]   = sum_A_q_in[m_load_idx];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 每个线程加载一个 B 的参数</span></span><br><span class="line">    <span class="type">int</span> n_load_idx = block_col_start + tx;</span><br><span class="line">    <span class="keyword">if</span> (ty == <span class="number">0</span> &amp;&amp; n_load_idx &lt; N) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">size_t</span> b_param_idx = n_load_idx * num_oc_groups + group_idx;</span><br><span class="line">        smem_scale_B[tx]  = (<span class="type">float</span>)base_scale_B[b_param_idx];</span><br><span class="line">        smem_offset_B[tx] = (<span class="type">float</span>)base_offset_B[b_param_idx];</span><br><span class="line">        smem_sum_B_q[tx]  = base_sum_B_q[b_param_idx];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算每个线程负责的输出点</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> m = block_row_start + ty;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> n = block_col_start + tx;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (m &lt; M &amp;&amp; n &lt; N) &#123;</span><br><span class="line">        <span class="comment">// 从共享内存中读取参数</span></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> scale_A = smem_scale_A[ty];</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> offset_A = smem_offset_A[ty];</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> sum_A_q = (<span class="type">float</span>)smem_sum_A_q[ty];</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> scale_B = smem_scale_B[tx];</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> offset_B = smem_offset_B[tx];</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> sum_B_q = (<span class="type">float</span>)smem_sum_B_q[tx];</span><br><span class="line">        </span><br><span class="line">        <span class="type">const</span> <span class="type">float</span> c_q_val = (<span class="type">float</span>)C_q[m * ldc + n];</span><br><span class="line"></span><br><span class="line">        <span class="type">float</span> term1 = scale_A * (c_q_val * scale_B + sum_A_q * offset_B);</span><br><span class="line">        <span class="type">float</span> term2 = offset_A * (sum_B_q * scale_B + K_i * offset_B);</span><br><span class="line">        <span class="type">float</span> final_val = term1 + term2;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">if</span> (defined(__CUDACC__) &amp;&amp; (!defined(__CUDA_ARCH__) || (__CUDA_ARCH__ &gt;= 700))) || defined(_NVHPC_CUDA)</span></span><br><span class="line">            <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(std::is_same_v&lt;T, <span class="type">float</span>&gt;)</span> </span>&#123;</span><br><span class="line">                <span class="built_in">atomicAdd</span>(&amp;C_fp_final[m * ldc + n], final_val);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> <span class="built_in">constexpr</span> (std::is_same_v&lt;T, half&gt;) &#123;</span><br><span class="line">                <span class="comment">// printf(&quot;[final_val]%.4f %.4f\n&quot;, __half2float(C_fp_final[m * ldc + n]), final_val);</span></span><br><span class="line">                <span class="built_in">atomicAdd</span>(&amp;C_fp_final[m * ldc + n], __float2half(final_val));</span><br><span class="line">                <span class="comment">// printf(&quot;[C_fp_final]%.4f\n&quot;, __half2float(C_fp_final[m * ldc + n]));</span></span><br><span class="line">            &#125;</span><br><span class="line">        <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">            C_fp_final[m * ldc + n] += final_val;</span><br><span class="line">        <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 预计算权重 B 的修正项 (sum_B_q)</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Precompute_SumBq</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int8_t</span>* B_q, <span class="type">int32_t</span>* sum_B_q_out,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> num_groups, <span class="type">const</span> <span class="type">int</span> ic_per_group,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> oc, <span class="type">const</span> <span class="type">int</span> ic_p</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 每个线程处理一个 (output_channel, group) 的修正项</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; (<span class="type">size_t</span>)oc * num_groups; index += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        </span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> i = index % num_groups; <span class="comment">// group_idx</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> n = index / num_groups; <span class="comment">// output_channel</span></span><br><span class="line">        </span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> k_start = i * ic_per_group;</span><br><span class="line">        </span><br><span class="line">        <span class="type">int32_t</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k_offset = <span class="number">0</span>; k_offset &lt; ic_per_group; ++k_offset) &#123;</span><br><span class="line">            <span class="comment">// 访问 B 矩阵的布局是 [oc][ic_p]</span></span><br><span class="line">            sum += (<span class="type">int32_t</span>)B_q[n * ic_p + k_start + k_offset];</span><br><span class="line">        &#125;</span><br><span class="line">        sum_B_q_out[index] = sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">BiasAndActivation</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    T* data, <span class="type">const</span> T* bias,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">float</span> minV, <span class="type">const</span> <span class="type">float</span> maxV,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M, <span class="type">const</span> <span class="type">int</span> N, <span class="type">const</span> <span class="type">int</span> ldc</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; (<span class="type">size_t</span>)M * N; index += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> m = index / N;</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span> n = index % N;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">size_t</span> buffer_idx = m * ldc + n;</span><br><span class="line">        <span class="type">float</span> val = (<span class="type">float</span>)data[buffer_idx];</span><br><span class="line">        val += (<span class="type">float</span>)bias[n];</span><br><span class="line">        val = <span class="built_in">max</span>(val, minV);</span><br><span class="line">        val = <span class="built_in">min</span>(val, maxV);</span><br><span class="line">        data[buffer_idx] = (T)val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里我们只对 INT8 进行优化，因为 Cutlass 对 INT4 GEMM
的支持与硬件计算能力绑定。</p>
<p>查阅 <a
target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html">PTX
ISA</a> 和 <a
target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/turing-tuning-guide/index.html">Turing
Guide</a> 可知，SM_75 架构（Turing）虽然 Tensor Core 理论支持 INT4
GEMM，也提供了诸如
<code>mma.sync.aligned.m8n8k16.row.col.s32.s4.s4.s32</code> 的 PTX
ISA，但 Cutlass 并没有很好地集成相应能力。而 s4.s4 的 wmma 和 Cutlass
支持则要到 Ampere（SM_80 以上）才能支持。</p>
<p>Cutlass 代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">ifndef</span> CutlassGemmIntParam_hpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CutlassGemmIntParam_hpp</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../CutlassGemmParam.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> MNN &#123;</span><br><span class="line"><span class="keyword">namespace</span> CUDA &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// int8 * int8 =&gt; int32 GEMM.</span></span><br><span class="line"><span class="keyword">using</span> EpilogueGemmInt = cutlass::epilogue::thread::LinearCombination&lt;</span><br><span class="line">    <span class="type">int32_t</span>, <span class="comment">// ElementC</span></span><br><span class="line">    <span class="number">1</span>,       <span class="comment">// Elements per access.</span></span><br><span class="line">    <span class="type">int32_t</span>, <span class="comment">// ElementAccumulator</span></span><br><span class="line">    <span class="type">int32_t</span>  <span class="comment">// ElementCompute, not used for this epilogue</span></span><br><span class="line">&gt;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> CutlassGemmInt = cutlass::gemm::device::Gemm&lt;</span><br><span class="line">    <span class="type">int8_t</span>,                             <span class="comment">// ElementA</span></span><br><span class="line">    cutlass::layout::RowMajor,          <span class="comment">// LayoutA</span></span><br><span class="line">    <span class="type">int8_t</span>,                             <span class="comment">// ElementB</span></span><br><span class="line">    cutlass::layout::ColumnMajor,       <span class="comment">// LayoutB (Using the same trick as before)</span></span><br><span class="line">    <span class="type">int32_t</span>,                            <span class="comment">// ElementC</span></span><br><span class="line">    cutlass::layout::RowMajor,          <span class="comment">// LayoutC</span></span><br><span class="line">    <span class="type">int32_t</span>,                            <span class="comment">// ElementAccumulator</span></span><br><span class="line">    cutlass::arch::OpClassTensorOp,</span><br><span class="line">    cutlass::arch::Sm75,                <span class="comment">// Target GPU Architecture</span></span><br><span class="line">    cutlass::gemm::GemmShape&lt;<span class="number">128</span>, <span class="number">128</span>, <span class="number">32</span>&gt;, <span class="comment">// ThreadblockShape</span></span><br><span class="line">    cutlass::gemm::GemmShape&lt;<span class="number">64</span>, <span class="number">64</span>, <span class="number">32</span>&gt;,  <span class="comment">// WarpShape</span></span><br><span class="line">    cutlass::gemm::GemmShape&lt;<span class="number">8</span>, <span class="number">8</span>, <span class="number">16</span>&gt;,    <span class="comment">// InstructionShape</span></span><br><span class="line">    EpilogueGemmInt,</span><br><span class="line">    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle&lt;&gt;, <span class="comment">// Swizzle</span></span><br><span class="line">    <span class="number">2</span> <span class="comment">// Stages</span></span><br><span class="line">&gt;;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace CUDA</span></span><br><span class="line">&#125; <span class="comment">// namespace MNN</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">// CutlassGemmIntParam_hpp</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 准备阶段</span></span><br><span class="line">mGemmArguments = &#123;</span><br><span class="line">    &#123;M, N, ic_per_group&#125;,</span><br><span class="line">    &#123;<span class="literal">nullptr</span>, ic_per_group&#125;, <span class="comment">// ptr_A and lda_A</span></span><br><span class="line">    &#123;<span class="literal">nullptr</span>, <span class="built_in">UP_DIV</span>(K, <span class="number">8</span>) * <span class="number">8</span>&#125;, <span class="comment">// ptr_B and ldb_B</span></span><br><span class="line">    &#123;<span class="literal">nullptr</span>, <span class="built_in">UP_DIV</span>(N, <span class="number">8</span>) * <span class="number">8</span>&#125;, <span class="comment">// ptr_C and ldc_C</span></span><br><span class="line">    &#123;<span class="literal">nullptr</span>, <span class="built_in">UP_DIV</span>(N, <span class="number">8</span>) * <span class="number">8</span>&#125;, <span class="comment">// ptr_D and ldd_D</span></span><br><span class="line">    &#123;<span class="number">1</span>, <span class="number">0</span>&#125;, <span class="comment">// Epilogue: D = 1 * A*B + 0 * C</span></span><br><span class="line">    <span class="number">1</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Check if CUTLASS can support this problem</span></span><br><span class="line">cutlass::Status status = mCutlassGemmInt.<span class="built_in">can_implement</span>(mGemmArguments);</span><br><span class="line"><span class="keyword">if</span> (status != cutlass::Status::kSuccess) &#123;</span><br><span class="line">    <span class="built_in">MNN_ERROR</span>(<span class="string">&quot;CUTLASS GEMM cannot implement this problem\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> NOT_SUPPORT;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocate workspace if needed</span></span><br><span class="line"><span class="type">size_t</span> workspace_size = mCutlassGemmInt.<span class="built_in">get_workspace_size</span>(mGemmArguments);</span><br><span class="line"><span class="keyword">if</span> (workspace_size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    mWorkspaceTensor.<span class="built_in">reset</span>(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">int8_t</span>&gt;(&#123;(<span class="type">int</span>)workspace_size&#125;));</span><br><span class="line">    <span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(mWorkspaceTensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line">    mWorkspacePtr = (<span class="type">void</span>*)mWorkspaceTensor-&gt;<span class="built_in">buffer</span>().device;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Initialize the GEMM kernel</span></span><br><span class="line">status = mCutlassGemmInt.<span class="built_in">initialize</span>(mGemmArguments, mWorkspacePtr);</span><br><span class="line"><span class="built_in">cutlass_check</span>(status);</span><br></pre></td></tr></table></figure>
<p>主函数：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> M = input-&gt;<span class="built_in">batch</span>();</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> K = input-&gt;<span class="built_in">channel</span>();</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = output-&gt;<span class="built_in">channel</span>();</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> ic_p = <span class="built_in">UP_DIV</span>(K, <span class="number">8</span>) * <span class="number">8</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> oc_p = <span class="built_in">UP_DIV</span>(N, <span class="number">8</span>) * <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> num_groups = (mResource-&gt;mQuanC &gt; <span class="number">0</span>) ? (mResource-&gt;mQuanC / N) : <span class="number">1</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> ic_per_group = (num_groups &gt; <span class="number">0</span>) ? (K / num_groups) : K;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> Cq_tensor = std::<span class="built_in">make_shared</span>&lt;Tensor&gt;(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">int32_t</span>&gt;(&#123;M, oc_p&#125;));</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(Cq_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="type">int32_t</span>* C_q_buffer = (<span class="type">int32_t</span>*)Cq_tensor-&gt;<span class="built_in">deviceId</span>();</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> type_size = mFp16Infer ? <span class="built_in">sizeof</span>(half) : <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="keyword">auto</span> scaleA_tensor = std::<span class="built_in">make_shared</span>&lt;Tensor&gt;(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">uint8_t</span>&gt;(&#123;M * type_size&#125;));</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(scaleA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="type">void</span>* scale_A_buffer = (<span class="type">void</span>*)scaleA_tensor-&gt;<span class="built_in">deviceId</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> offsetA_tensor = std::<span class="built_in">make_shared</span>&lt;Tensor&gt;(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">uint8_t</span>&gt;(&#123;M * type_size&#125;));</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(offsetA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="type">void</span>* offset_A_buffer = (<span class="type">void</span>*)offsetA_tensor-&gt;<span class="built_in">deviceId</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> sumA_tensor = std::<span class="built_in">make_shared</span>&lt;Tensor&gt;(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">int32_t</span>&gt;(&#123;M&#125;));</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(sumA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="type">int32_t</span>* sum_A_q_buffer = (<span class="type">int32_t</span>*)sumA_tensor-&gt;<span class="built_in">deviceId</span>();</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> Aq_tensor = std::<span class="built_in">make_shared</span>&lt;Tensor&gt;(Tensor::<span class="built_in">createDevice</span>&lt;<span class="type">int8_t</span>&gt;(&#123;M, ic_per_group&#125;));</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onAcquireBuffer</span>(Aq_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="type">int8_t</span>* A_q_buffer = (<span class="type">int8_t</span>*)Aq_tensor-&gt;<span class="built_in">deviceId</span>();</span><br><span class="line"></span><br><span class="line">runtime-&gt;<span class="built_in">memset</span>(output_addr, <span class="number">0</span>, M * N * type_size);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 按 K 维度块循环执行</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_groups; ++i) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> k_start = i * ic_per_group;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// QuantA</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blocks_quant</span><span class="params">(M)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">threads_quant</span><span class="params">(BLOCK_SIZE)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// GEMM_Int8</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blocks_gemm</span><span class="params">(UP_DIV(oc_p, SUB_GEMM_TILE_DIM), UP_DIV(M, SUB_GEMM_TILE_DIM))</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">threads_gemm</span><span class="params">(SUB_GEMM_TILE_DIM, SUB_GEMM_TILE_DIM)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// DequantAndAcc</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blocks_dequant</span><span class="params">(UP_DIV(N, DEQUANT_TILE_DIM), UP_DIV(M, DEQUANT_TILE_DIM))</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">threads_dequant</span><span class="params">(DEQUANT_TILE_DIM, DEQUANT_TILE_DIM)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mFp16Infer) &#123;</span><br><span class="line">        <span class="type">const</span> half* A_sub_ptr = (<span class="type">const</span> half*)input_addr + k_start;</span><br><span class="line">        <span class="type">const</span> <span class="type">int8_t</span>* B_sub_ptr = (<span class="type">const</span> <span class="type">int8_t</span>*)mResource-&gt;mFilter + k_start;</span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> half* base_scale_B_ptr = (<span class="type">const</span> half*)mResource-&gt;mScale;</span><br><span class="line">        <span class="type">const</span> half* base_offset_B_ptr = (<span class="type">const</span> half*)mResource-&gt;mOffset;</span><br><span class="line">        <span class="type">const</span> <span class="type">int32_t</span>* base_sum_B_q_ptr = (<span class="type">const</span> <span class="type">int32_t</span>*)mResource-&gt;mSumBQ;</span><br><span class="line"></span><br><span class="line">        QuantA&lt;half&gt;&lt;&lt;&lt;blocks_quant, threads_quant&gt;&gt;&gt;(</span><br><span class="line">            A_sub_ptr, A_q_buffer,</span><br><span class="line">            (half*)scale_A_buffer, (half*)offset_A_buffer, sum_A_q_buffer,</span><br><span class="line">            M, ic_per_group, ic_p</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// GEMM_Int8&lt;&lt;&lt;blocks_gemm, threads_gemm&gt;&gt;&gt;(</span></span><br><span class="line">        <span class="comment">//     A_q_buffer, B_sub_ptr, C_q_buffer,</span></span><br><span class="line">        <span class="comment">//     M, N, ic_per_group, ic_per_group, ic_p, oc_p</span></span><br><span class="line">        <span class="comment">// );</span></span><br><span class="line">        </span><br><span class="line">        mGemmArguments.ref_A.<span class="built_in">reset</span>(A_q_buffer);</span><br><span class="line">        mGemmArguments.ref_B.<span class="built_in">reset</span>(B_sub_ptr);</span><br><span class="line">        mGemmArguments.ref_C.<span class="built_in">reset</span>(C_q_buffer);</span><br><span class="line">        mGemmArguments.ref_D.<span class="built_in">reset</span>(C_q_buffer);</span><br><span class="line">        </span><br><span class="line">        cutlass::Status status = <span class="built_in">mCutlassGemmInt</span>(mGemmArguments, mWorkspacePtr, <span class="number">0</span>);</span><br><span class="line">        <span class="built_in">cutlass_check</span>(status);</span><br><span class="line">        </span><br><span class="line">        DequantAndAcc&lt;half&gt;&lt;&lt;&lt;blocks_dequant, threads_dequant&gt;&gt;&gt;(</span><br><span class="line">            C_q_buffer, (half*)output_addr,</span><br><span class="line">            (<span class="type">const</span> half*)scale_A_buffer, (<span class="type">const</span> half*)offset_A_buffer,</span><br><span class="line">            base_scale_B_ptr, base_offset_B_ptr,</span><br><span class="line">            base_sum_B_q_ptr,</span><br><span class="line">            i, num_groups,</span><br><span class="line">            sum_A_q_buffer,</span><br><span class="line">            M, N, ic_per_group, oc_p</span><br><span class="line">        );</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// FP32</span></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span>* A_sub_ptr = (<span class="type">const</span> <span class="type">float</span>*)input_addr + k_start; <span class="comment">// A 第 k 块的起始位置</span></span><br><span class="line">        <span class="type">const</span> <span class="type">int8_t</span>* B_sub_ptr = (<span class="type">const</span> <span class="type">int8_t</span>*)mResource-&gt;mFilter + k_start; <span class="comment">// B 第 k 块的起始位置</span></span><br><span class="line"></span><br><span class="line">        <span class="type">const</span> <span class="type">float</span>* base_scale_B_ptr = (<span class="type">const</span> <span class="type">float</span>*)mResource-&gt;mScale;</span><br><span class="line">        <span class="type">const</span> <span class="type">float</span>* base_offset_B_ptr = (<span class="type">const</span> <span class="type">float</span>*)mResource-&gt;mOffset;</span><br><span class="line">        <span class="type">const</span> <span class="type">int32_t</span>* base_sum_B_q_ptr = (<span class="type">const</span> <span class="type">int32_t</span>*)mResource-&gt;mSumBQ;</span><br><span class="line"></span><br><span class="line">        QuantA&lt;<span class="type">float</span>&gt;&lt;&lt;&lt;blocks_quant, threads_quant&gt;&gt;&gt;(</span><br><span class="line">            A_sub_ptr, A_q_buffer,</span><br><span class="line">            (<span class="type">float</span>*)scale_A_buffer, (<span class="type">float</span>*)offset_A_buffer, sum_A_q_buffer,</span><br><span class="line">            M, ic_per_group, ic_p</span><br><span class="line">        );</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// GEMM_Int8&lt;&lt;&lt;blocks_gemm, threads_gemm&gt;&gt;&gt;(</span></span><br><span class="line">        <span class="comment">//     A_q_buffer, B_sub_ptr, C_q_buffer,</span></span><br><span class="line">        <span class="comment">//     M, N, ic_per_group, ic_per_group, ic_p, oc_p</span></span><br><span class="line">        <span class="comment">// );</span></span><br><span class="line">        </span><br><span class="line">        mGemmArguments.ref_A.<span class="built_in">reset</span>(A_q_buffer);</span><br><span class="line">        mGemmArguments.ref_B.<span class="built_in">reset</span>(B_sub_ptr);</span><br><span class="line">        mGemmArguments.ref_C.<span class="built_in">reset</span>(C_q_buffer);</span><br><span class="line">        mGemmArguments.ref_D.<span class="built_in">reset</span>(C_q_buffer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Run the pre-initialized GEMM operation</span></span><br><span class="line">        cutlass::Status status = <span class="built_in">mCutlassGemmInt</span>(mGemmArguments, mWorkspacePtr, <span class="number">0</span>);</span><br><span class="line">        <span class="built_in">cutlass_check</span>(status);</span><br><span class="line">        </span><br><span class="line">        DequantAndAcc&lt;<span class="type">float</span>&gt;&lt;&lt;&lt;blocks_dequant, threads_dequant&gt;&gt;&gt;(</span><br><span class="line">            C_q_buffer, (<span class="type">float</span>*)output_addr,</span><br><span class="line">            (<span class="type">const</span> <span class="type">float</span>*)scale_A_buffer, (<span class="type">const</span> <span class="type">float</span>*)offset_A_buffer,</span><br><span class="line">            base_scale_B_ptr, base_offset_B_ptr,</span><br><span class="line">            base_sum_B_q_ptr,</span><br><span class="line">            i, num_groups,</span><br><span class="line">            sum_A_q_buffer,</span><br><span class="line">            M, N, ic_per_group, oc_p</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> maxV = FLT_MAX, minV = -FLT_MAX;</span><br><span class="line"><span class="keyword">if</span> (mActivationType == <span class="number">1</span>) minV = <span class="number">0.0f</span>;</span><br><span class="line"><span class="keyword">if</span> (mActivationType == <span class="number">2</span>) &#123; minV = <span class="number">0.0f</span>; maxV = <span class="number">6.0f</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> total_threads_act = M * oc_p;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> block_size_act = BLOCK_SIZE;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> num_blocks_act = (total_threads_act + block_size_act - <span class="number">1</span>) / block_size_act;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (mFp16Infer) &#123;</span><br><span class="line">    BiasAndActivation&lt;half&gt;&lt;&lt;&lt;num_blocks_act, block_size_act&gt;&gt;&gt;((half*)output_addr, (<span class="type">const</span> half*)mResource-&gt;mBias, minV, maxV, M, N, oc_p);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    BiasAndActivation&lt;<span class="type">float</span>&gt;&lt;&lt;&lt;num_blocks_act, block_size_act&gt;&gt;&gt;((<span class="type">float</span>*)output_addr, (<span class="type">const</span> <span class="type">float</span>*)mResource-&gt;mBias, minV, maxV, M, N, oc_p);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onReleaseBuffer</span>(Cq_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onReleaseBuffer</span>(scaleA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onReleaseBuffer</span>(offsetA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onReleaseBuffer</span>(sumA_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"><span class="built_in">backend</span>()-&gt;<span class="built_in">onReleaseBuffer</span>(Aq_tensor.<span class="built_in">get</span>(), Backend::STATIC);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> NO_ERROR;</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/04/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" rel="prev" title="强化学习入门笔记">
      <i class="fa fa-chevron-left"></i> 强化学习入门笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/10/03/%E5%A4%AA%E7%A9%BA%E4%BA%BA-%E8%A1%8C%E5%89%8D%E7%AF%87/" rel="next" title="太空人-行前篇">
      太空人-行前篇 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="nav-number">1.</span> <span class="nav-text">问题定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9D%E6%8E%A2"><span class="nav-number">2.</span> <span class="nav-text">初探</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#conv_fpaint4b"><span class="nav-number">2.1.</span> <span class="nav-text">CONV_FpAInt4B</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#convint8cutlassexecution"><span class="nav-number">2.2.</span> <span class="nav-text">ConvInt8CutlassExecution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#llama.cpp-%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.3.</span> <span class="nav-text">llama.cpp 实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88%E4%B8%80%E5%9C%A8%E7%BA%BF%E5%8F%8D%E9%87%8F%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">方案一：在线反量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88%E4%BA%8C%E5%9C%A8%E7%BA%BF%E9%87%8F%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">方案二：在线量化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%A1%88%E6%8E%A8%E5%AF%BC"><span class="nav-number">4.1.</span> <span class="nav-text">方案推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">4.2.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="HocRiser"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">HocRiser</p>
  <div class="site-description" itemprop="description">平凡即是喜乐</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/HocRiser01" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HocRiser01" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hocriser@gmail.com" title="E-Mail → mailto:hocriser@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/GGN_2015" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;GGN_2015" rel="noopener" target="_blank">GGN_2015</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yirannn.com/" title="https:&#x2F;&#x2F;yirannn.com&#x2F;" rel="noopener" target="_blank">Yirannn</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://oceanpresent.art/" title="https:&#x2F;&#x2F;oceanpresent.art&#x2F;" rel="noopener" target="_blank">OceanPresent</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://atoposyz.github.io/" title="https:&#x2F;&#x2F;atoposyz.github.io&#x2F;" rel="noopener" target="_blank">Atoposyz</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">皖ICP备2023012352号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">HocRiser</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">209k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">6:20</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
